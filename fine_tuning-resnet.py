# -*- coding: utf-8 -*-
"""Pary-B.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bpoHnP6jovW9WLpw3MbTsJb9cuoAOCQ-
"""

!pip install wandb

from google.colab import drive
drive.mount('/content/drive')

zip_path = "drive/MyDrive/nature_12K.zip"
!cp "{zip_path}" .
!unzip -q nature_12K.zip
!rm nature_12K.zip

import torch
import torchvision
from torchvision import datasets, transforms
from torch import nn, optim
from torch.nn import functional as F
from torch.utils.data import DataLoader, sampler, random_split
from torchvision import models
import wandb

import os
import glob
import matplotlib.pyplot as plt
import numpy as np

import gc

def prepare_datasets(batch_size,directory="inaturalist_12K",data_augmentation=False,test=False):
  train_directory=os.path.join(directory,"train")
  test_directory=os.path.join(directory,"val")
  generator = torch.Generator().manual_seed(42)
  if data_augmentation==True:
    transform = transforms.Compose([
            transforms.RandomHorizontalFlip(p=0.4),
            transforms.RandomVerticalFlip(p=0.3),
            transforms.RandomApply(transforms=[
                transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))
            ], p=0.43),
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        ])
    all_train=datasets.ImageFolder(train_directory,transform=transform)
    train_data_len = int(len(all_train)*0.8)
    valid_data_len = int(len(all_train)-(train_data_len))

    train_data,val_data=random_split(all_train,[train_data_len,valid_data_len],generator.manual_seed(42))
    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False) # shuffle=True?
    val_loader=DataLoader(val_data,batch_size=batch_size,shuffle=False)

    if test==True:
        transform = transforms.Compose([
            transforms.RandomHorizontalFlip(p=0.4),
            transforms.RandomVerticalFlip(p=0.3),
            transforms.RandomApply(transforms=[
                transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))
            ], p=0.43),
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        ])
        all_train=datasets.ImageFolder(train_directory,transform=transform)
        train_loader = DataLoader(all_train, batch_size=batch_size, shuffle=False) # shuffle=True?

        test_transform=transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        ])
        all_test=datasets.ImageFolder(test_directory,transform=transform)
        test_data_len=len(all_test)
        test_loader=DataLoader(all_test,batch_size=batch_size,shuffle=False)

        return train_loader, test_loader, len(all_train), test_data_len


  else:
    transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        ])

    all_train=datasets.ImageFolder(train_directory,transform=transform)
    train_data_len = int(len(all_train)*0.8)
    valid_data_len = int(len(all_train)-(train_data_len))

    train_data,val_data=random_split(all_train,[train_data_len,valid_data_len],generator.manual_seed(42))
    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False) # shuffle=True?
    val_loader=DataLoader(val_data,batch_size=batch_size,shuffle=False)

    if test==True:
        transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        ])
        all_train=datasets.ImageFolder(train_directory,transform=transform)
        train_loader = DataLoader(all_train, batch_size=batch_size, shuffle=False) # shuffle=True?


        test_transform=transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        ])
        all_test=datasets.ImageFolder(test_directory,transform=transform)
        test_data_len=len(all_test)
        test_loader=DataLoader(all_test,batch_size=batch_size,shuffle=False)

        return train_loader, test_loader, len(all_train), test_data_len

  return train_loader,val_loader, train_data_len, valid_data_len

train,val,num_train,num_val=prepare_datasets(32,data_augmentation=False)

dataiter = iter(train)
images, labels = next(dataiter)
print(labels)
plt.imshow(np.transpose(torchvision.utils.make_grid(
  images[20], normalize=True, padding=1, nrow=5).numpy(), (1, 2, 0)))
plt.axis('off')

"""# Fine-tuning

"""

model=models.googlenet(pretrained=True)

model

def finetune_model(model_name,neurons_dense,activation_function):



  if model_name == "ResNet50":
    model=models.resnet50(pretrained=True)

    for params in model.parameters():
      params.requires_grad = False

    if activation_function == "relu":
        activation_function_layer=nn.ReLU()
    elif activation_function == "gelu":
        activation_function_layer=nn.GELU()
    elif activation_function == "silu":
        activation_function_layer=nn.SiLU()
    elif activation_function == "mish":
        activation_function_layer=nn.Mish()


    n_inputs = model.fc.in_features
    model.fc = nn.Sequential(
      nn.Linear(n_inputs,neurons_dense),
      activation_function_layer,
      nn.Dropout(0.2),
      nn.Linear(neurons_dense, 10)
    )

    return model


  elif model_name == "EfficientNetV2":
    model=models.efficientnet_v2_m(pretrained=True)

    for params in model.parameters():
      params.requires_grad = False

    if activation_function == "relu":
        activation_function_layer=nn.ReLU()
    elif activation_function == "gelu":
        activation_function_layer=nn.GELU()
    elif activation_function == "silu":
        activation_function_layer=nn.SiLU()
    elif activation_function == "mish":
        activation_function_layer=nn.Mish()

    n_inputs = model.classifier[1].in_features

    model.classifier = nn.Sequential(
      nn.Linear(n_inputs,neurons_dense),
      activation_function_layer,
      nn.Dropout(0.2),
      nn.Linear(neurons_dense, 10)
    )

    return model

  elif model_name == "GoogLeNet":
    model=models.googlenet(pretrained=True)

    for params in model.parameters():
      params.requires_grad = False

    if activation_function == "relu":
        activation_function_layer=nn.ReLU()
    elif activation_function == "gelu":
        activation_function_layer=nn.GELU()
    elif activation_function == "silu":
        activation_function_layer=nn.SiLU()
    elif activation_function == "mish":
        activation_function_layer=nn.Mish()


    n_inputs = model.fc.in_features
    model.fc = nn.Sequential(
      nn.Linear(n_inputs,neurons_dense),
      activation_function_layer,
      nn.Dropout(0.2),
      nn.Linear(neurons_dense, 10)
    )

    return model

#model=finetune_model("EfficientNetV2",256,"mish")

class EarlyStopper:
    def __init__(self, patience=3, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.min_validation_loss = np.inf

    def early_stop(self, validation_loss):
        if validation_loss < self.min_validation_loss:
            self.min_validation_loss = validation_loss
            self.counter = 0
        elif validation_loss > (self.min_validation_loss + self.min_delta):
            self.counter += 1
            if self.counter >= self.patience:
                return True
        return False

def wandb_sweep():
      # Default values for hyper-parameters
    config_defaults = {
        'model_name':'InceptionV3',
        'activation_function': "gelu",
        'neurons_dense': 256,
        'batch_size': 32,
        'data_augmentation': True,
        'num_epochs':5
    }

    # Initialize a new wandb run
    wandb.init(config=config_defaults,resume=True)

    # Config saves hyperparameters and inputs
    config = wandb.config

    activation_function=config.activation_function
    model_name=config.model_name
    batch_size=config.batch_size
    neurons_dense=config.neurons_dense
    data_augmentation=config.data_augmentation
    num_epochs=config.num_epochs

    es = EarlyStopper()

    # Display the hyperparameters
    run_name = f"model_{model_name}_af_{activation_function}_bs_{batch_size}_neurons_{neurons_dense}_data_aug_{data_augmentation}_ep_{num_epochs}"
    print(run_name)


    model = finetune_model(model_name=model_name,neurons_dense=neurons_dense,activation_function=activation_function)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.0001)

    train,val,num_train,num_val=prepare_datasets(batch_size,data_augmentation=data_augmentation)

    train_loss_list=[]
    val_loss_list=[]

    for epoch in range(num_epochs):

      train_loss=0
      val_loss=0
      total_train_correct=0
      total_val_correct=0

      model.train()
      for images, labels in train:

        #Extracting images and target labels for the batch being iterated
        images = images.to(device)
        labels = labels.to(device)

        #Calculating the model output and the cross entropy loss
        #train_outputs = model(images)
        #train_pred=torch.argmax(train_outputs,dim=1)
        #correct_train_pred=sum(train_pred==labels).item()
        #total_train_correct+=correct_train_pred
        #loss = criterion(train_outputs, labels)

        #Updating weights according to calculated loss
        optimizer.zero_grad()
        with torch.cuda.amp.autocast():
          train_outputs = model(images)
          loss = criterion(train_outputs, labels)

        loss.backward()
        optimizer.step()

        train_pred=torch.argmax(train_outputs,dim=1)
        correct_train_pred=sum(train_pred==labels).item()
        total_train_correct+=correct_train_pred
        train_loss += loss.item()*images.size(0)

      del images
      del labels

      model.eval()
      for images,labels in val:

        images = images.to(device)
        labels = labels.to(device)

        val_output= model(images)

        loss= criterion(val_output,labels)
        val_pred=torch.argmax(val_output,dim=1)
        correct_val_pred=sum(val_pred==labels).item()
        total_val_correct+=correct_val_pred

        val_loss += loss.item() * images.size(0)

      del images
      del labels

      train_accuracy=total_train_correct/len(train.sampler)
      val_accuracy=total_val_correct/len(val.sampler)

      train_loss = train_loss/len(train.sampler)
      val_loss = val_loss/len(val.sampler)
      train_loss_list.append(train_loss)
      val_loss_list.append(val_loss)

      wandb.log({"training_acc": train_accuracy, "validation_accuracy": val_accuracy, "training_loss": train_loss, "validation loss": val_loss, "Epoch": epoch+1})

      print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f} \tTraining Accuracy: {:.6f} \tValidation Accuracy: {:.6f}'.format(
        epoch+1, train_loss, val_loss, train_accuracy, val_accuracy))

      if es.early_stop(val_loss):
        break



    wandb.run.name = run_name
    wandb.run.save()
    wandb.run.finish()

    del model
    del criterion
    del optimizer
    del train_loss_list
    del val_loss_list
    del train
    del val
    del train_accuracy
    del val_accuracy
    torch.cuda.empty_cache()
    gc.collect()

    #plt.plot(train_loss_list, label='Training loss')
    #plt.plot(val_loss_list, label='Validation loss')
    #plt.xlabel("Epochs")
    #plt.ylabel("Loss")
    #plt.legend(frameon=False)

sweep_config = {
  "name": "Assignment2(Part-B)",
  "method": "bayes",
  "metric": {
      "name":"validation_accuracy",
      "goal": "maximize"
  },
  "parameters": {
        "activation_function": {
            "values": ['relu','gelu','silu','mish']
        },
        "model_name": {
            "values": ["InceptionV3","ResNet50","EfficientNetV2","GoogLeNet"]
        },
        "batch_size": {
            "values": [16,32]
        },
        "neurons_dense": {
            "values": [256,512,768]
        },
        "data_augmentation":{
            "values": [True,False]
        },
        "num_epochs":{
            "values": [5,10]
        }
    }
}

sweep_id = wandb.sweep(sweep_config,  entity="shashwat_mm19b053", project="Assignment-2")
wandb.agent("n37tk8ni",project='Assignment-2', function=wandb_sweep, count=30)